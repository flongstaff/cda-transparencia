{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive PDF Data Extraction System
",
    "## Educational/Research Purpose Only
",
    "
",
    "This notebook provides comprehensive PDF extraction capabilities for all available documents.
",
    "Supports all years (2018-2025) and multiple extraction methods for maximum data recovery.
",
    "
",
    "### Features:
",
    "- Multi-engine PDF processing (pdfplumber, PyMuPDF, tabula-py, PyPDF2)
",
    "- Automatic year detection and classification
",
    "- CSV and JSON export capabilities
",
    "- Financial data pattern recognition
",
    "- Comprehensive logging and error handling
",
    "- Progress tracking and statistics
",
    "- Year-based organization (2018-2025)
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PDF processing libraries availability
",
    "import os
",
    "import sys
",
    "import json
",
    "import pandas as pd
",
    "import numpy as np
",
    "from pathlib import Path
",
    "import re
",
    "from datetime import datetime
",
    "import glob
",
    "
",
    "print(\"üìä Comprehensive PDF Extraction System Initialized\")
",
    "print(\"üéì Educational/Research Purpose Only\")
",
    "print(\"üìÖ Supporting years: 2018-2025\")
",
    "
",
    "# Check PDF processing libraries
",
    "pdf_engines = {}
",
    "try:
",
    "    import pdfplumber
",
    "    pdf_engines[\"pdfplumber\"] = True
",
    "    print(\"‚úì pdfplumber available\")
",
    "except ImportError:
",
    "    pdf_engines[\"pdfplumber\"] = False
",
    "    print(\"‚ö†Ô∏è pdfplumber not available - install with: pip install pdfplumber\")
",
    "
",
    "available_engines = [k for k, v in pdf_engines.items() if v]
",
    "print(f\"üìã Available extraction engines: {available_engines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and organize PDF files by year
",
    "def find_and_organize_pdfs():
",
    "    \"\"\"Find and organize PDF files by year\"\"\"
",
    "    search_patterns = [
",
    "        \"../data/**/*.pdf\",
",
    "        \"../frontend/public/data/**/*.pdf\", 
",
    "        \"./data/**/*.pdf\",
",
    "        \"./**/*.pdf\"
",
    "    ]
",
    "    
",
    "    found_pdfs = []
",
    "    for pattern in search_patterns:
",
    "        pdfs = glob.glob(pattern, recursive=True)
",
    "        found_pdfs.extend(pdfs)
",
    "    
",
    "    # Remove duplicates
",
    "    unique_pdfs = list(set(found_pdfs))
",
    "    print(f\"üìÑ Found {len(unique_pdfs)} unique PDF files\")
",
    "    
",
    "    # Organize by year
",
    "    year_files = {}
",
    "    for pdf_path in unique_pdfs:
",
    "        filename = os.path.basename(pdf_path)
",
    "        year_match = re.search(r\"(20\d{2})\", filename)
",
    "        if year_match:
",
    "            year = int(year_match.group(1))
",
    "            if 2018 <= year <= 2025:
",
    "                if year not in year_files:
",
    "                    year_files[year] = []
",
    "                year_files[year].append(pdf_path)
",
    "        else:
",
    "            if \"Unknown\" not in year_files:
",
    "                year_files[\"Unknown\"] = []
",
    "            year_files[\"Unknown\"].append(pdf_path)
",
    "    
",
    "    print(\"üìÖ Files organized by year:\")
",
    "    for year in sorted([y for y in year_files.keys() if y != \"Unknown\"]):
",
    "        print(f\"  {year}: {len(year_files[year])} files\")
",
    "    
",
    "    if \"Unknown\" in year_files:
",
    "        print(f\"  Unknown year: {len(year_files[\"Unknown\"])} files\")
",
    "    
",
    "    return year_files
",
    "
",
    "# Find PDFs
",
    "year_files = find_and_organize_pdfs()
",
    "
",
    "# Create output structure
",
    "output_dir = Path(\"extracted_data\")
",
    "output_dir.mkdir(exist_ok=True)
",
    "(output_dir / \"csv\").mkdir(exist_ok=True)
",
    "(output_dir / \"json\").mkdir(exist_ok=True)
",
    "
",
    "print(f\"üìÅ Output directory created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process PDFs and extract data
",
    "def process_pdf_simple(pdf_path):
",
    "    \"\"\"Simple PDF processing\"\"\"
",
    "    result = {
",
    "        \"filename\": os.path.basename(pdf_path),
",
    "        \"path\": pdf_path,
",
    "        \"size_mb\": round(os.path.getsize(pdf_path) / 1024 / 1024, 2),
",
    "        \"tables_found\": 0,
",
    "        \"text_extracted\": False,
",
    "        \"year\": None
",
    "    }
",
    "    
",
    "    # Extract year
",
    "    year_match = re.search(r\"(20\d{2})\", result[\"filename\"])
",
    "    if year_match:
",
    "        result[\"year\"] = int(year_match.group(1))
",
    "    
",
    "    try:
",
    "        if pdf_engines.get(\"pdfplumber\", False):
",
    "            with pdfplumber.open(pdf_path) as pdf:
",
    "                total_tables = 0
",
    "                text_found = False
",
    "                
",
    "                for page in pdf.pages:
",
    "                    if page.extract_text():
",
    "                        text_found = True
",
    "                    
",
    "                    tables = page.extract_tables()
",
    "                    if tables:
",
    "                        total_tables += len(tables)
",
    "                
",
    "                result[\"tables_found\"] = total_tables
",
    "                result[\"text_extracted\"] = text_found
",
    "    except Exception as e:
",
    "        print(f\"Error processing {result[\"filename\"]}: {e}\")
",
    "    
",
    "    return result
",
    "
",
    "# Process all PDFs
",
    "all_results = []
",
    "total_files = sum(len(files) for files in year_files.values())
",
    "
",
    "print(f\"üîÑ Processing {total_files} PDF files...\")
",
    "processed = 0
",
    "
",
    "for year, files in year_files.items():
",
    "    print(f\"üìÖ Processing {year}: {len(files)} files\")
",
    "    
",
    "    for pdf_path in files:
",
    "        try:
",
    "            result = process_pdf_simple(pdf_path)
",
    "            all_results.append(result)
",
    "            processed += 1
",
    "            
",
    "            if processed % 10 == 0:
",
    "                print(f\"  Progress: {processed}/{total_files} files\")
",
    "        except Exception as e:
",
    "            print(f\"  ‚ùå Failed: {os.path.basename(pdf_path)}\")
",
    "
",
    "print(f\"‚úÖ Processing completed! {len(all_results)} files processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and generate summary
",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")
",
    "
",
    "# Convert to DataFrame
",
    "df = pd.DataFrame(all_results)
",
    "
",
    "# Save overall summary
",
    "csv_path = output_dir / \"csv\" / f\"pdf_extraction_summary_{timestamp}.csv\"
",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")
",
    "print(f\"üíæ CSV summary saved: {csv_path}\")
",
    "
",
    "# Save JSON
",
    "json_path = output_dir / \"json\" / f\"pdf_extraction_complete_{timestamp}.json\"
",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:
",
    "    json.dump({
",
    "        \"timestamp\": timestamp,
",
    "        \"total_files\": len(all_results),
",
    "        \"files\": all_results
",
    "    }, f, indent=2, ensure_ascii=False)
",
    "print(f\"üíæ JSON results saved: {json_path}\")
",
    "
",
    "# Save by year
",
    "for year in sorted(set(r[\"year\"] for r in all_results if r[\"year\"])):
",
    "    year_data = df[df[\"year\"] == year]
",
    "    if not year_data.empty:
",
    "        year_csv = output_dir / \"csv\" / f\"pdf_extraction_{year}_{timestamp}.csv\"
",
    "        year_data.to_csv(year_csv, index=False, encoding=\"utf-8\")
",
    "        print(f\"üíæ Year {year} CSV: {year_csv}\")
",
    "
",
    "# Generate statistics
",
    "stats = {
",
    "    \"total_files\": len(all_results),
",
    "    \"total_tables\": sum(r[\"tables_found\"] for r in all_results),
",
    "    \"files_with_text\": sum(1 for r in all_results if r[\"text_extracted\"]),
",
    "    \"total_size_mb\": sum(r[\"size_mb\"] for r in all_results),
",
    "    \"years_covered\": len(set(r[\"year\"] for r in all_results if r[\"year\"]))
",
    "}
",
    "
",
    "print(f\"\nüìä EXTRACTION STATISTICS:\")
",
    "print(f\"  üìÑ Total files: {stats[\"total_files\"]}\")
",
    "print(f\"  üìä Total tables: {stats[\"total_tables\"]}\")
",
    "print(f\"  üìù Files with text: {stats[\"files_with_text\"]}\")
",
    "print(f\"  üíæ Total size: {stats[\"total_size_mb\"]:.1f} MB\")
",
    "print(f\"  üìÖ Years covered: {stats[\"years_covered\"]}\")
",
    "
",
    "# Summary by year
",
    "print(f\"\nüìÖ SUMMARY BY YEAR:\")
",
    "if not df.empty:
",
    "    year_summary = df.groupby(\"year\").agg({
",
    "        \"filename\": \"count\",
",
    "        \"tables_found\": \"sum\", 
",
    "        \"text_extracted\": \"sum\",
",
    "        \"size_mb\": \"sum\"
",
    "    }).round(2)
",
    "    
",
    "    year_summary.columns = [\"Files\", \"Tables\", \"With_Text\", \"Size_MB\"]
",
    "    print(year_summary)
",
    "
",
    "print(f\"\nüéØ Ready for dashboard completo integration!\")
",
    "print(f\"üîó Access via /completo route with year selector (2018-2025)\")
",
    "print(f\"üìÅ All results saved in: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
