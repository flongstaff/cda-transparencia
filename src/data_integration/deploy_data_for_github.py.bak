#!/usr/bin/env python3
"""
Data Deployment Script for Carmen de Areco Transparency Portal

This script prepares the processed data for deployment to GitHub Pages and Cloudflare.
"""

import json
import shutil
from pathlib import Path
import pandas as pd
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataDeployment:
    """Class to prepare data for deployment"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.processed_data_dir = self.base_dir / \"data\" / \"processed\"
        self.src_processed_dir = self.base_dir / \"src\" / \"data\" / \"processed\"
        self.frontend_data_dir = self.base_dir / \"frontend\" / \"src\" / \"data\"
        
        # Define the data that should be available for the frontend
        self.data_files_to_deploy = [
            \"budget_execution_all_years.csv\",
            \"indicators_all_years.csv\",
            \"budget_execution_all_years.json\",
            \"indicators_all_years.json\"
        ]

    def prepare_data_for_frontend(self):
        """Prepare data in formats that the frontend can use"""
        logger.info(\"Preparing data for frontend...\")
        
        # If we have processed budget data, convert to JSON for frontend
        budget_csv_path = self.processed_data_dir / \"budget_execution_all_years.csv\"
        if budget_csv_path.exists():
            try:
                df = pd.read_csv(budget_csv_path)
                # Convert to JSON
                json_path = self.processed_data_dir / \"budget_execution_all_years.json\"
                df.to_json(json_path, orient='records', indent=2)
                logger.info(f\"Converted budget data to JSON: {json_path}\")
            except Exception as e:
                logger.error(f\"Error converting budget data to JSON: {e}\")
                
        # If we have processed indicators data, convert to JSON for frontend
        indicators_csv_path = self.processed_data_dir / \"indicators_all_years.csv\"
        if indicators_csv_path.exists():
            try:
                df = pd.read_csv(indicators_csv_path)
                # Convert to JSON
                json_path = self.processed_data_dir / \"indicators_all_years.json\"
                df.to_json(json_path, orient='records', indent=2)
                logger.info(f\"Converted indicators data to JSON: {json_path}\")
            except Exception as e:
                logger.error(f\"Error converting indicators data to JSON: {e}\")

    def update_frontend_data_index(self):
        """Update the comprehensive data index with latest information"""
        logger.info(\"Updating frontend data index...\")
        
        # Load the existing comprehensive data index
        index_path = self.frontend_data_dir / \"comprehensive_data_index.json\"
        if index_path.exists():
            with open(index_path, 'r', encoding='utf-8') as f:
                data_index = json.load(f)
        else:
            # Create a default structure
            data_index = {
                \"year\": 2025,
                \"municipality\": \"Carmen de Areco\",
                \"generated_at\": \"2025-01-10T15:30:00Z\",
                \"data_sources\": {},
                \"financialOverview\": {},
                \"budgetBreakdown\": [],
                \"documents\": [],
                \"dashboard\": {},
                \"spendingEfficiency\": {},
                \"auditOverview\": {},
                \"antiCorruption\": {},
                \"analysis\": {},
                \"consistency_check\": {},
                \"summary\": {},
                \"last_updated\": \"2025-01-10T15:30:00Z\"
            }
        
        # Update with information about available data
        data_index[\"data_sources\"][\"processed_data\"] = {
            \"type\": \"processed_municipal_data\",
            \"description\": \"Processed municipal data from transparency portal\",
            \"budget_execution_csv\": \"budget_execution_all_years.csv\" if (self.processed_data_dir / \"budget_execution_all_years.csv\").exists() else None,
            \"indicators_csv\": \"indicators_all_years.csv\" if (self.processed_data_dir / \"indicators_all_years.csv\").exists() else None,
            \"budget_execution_json\": \"budget_execution_all_years.json\" if (self.processed_data_dir / \"budget_execution_all_years.json\").exists() else None,
            \"indicators_json\": \"indicators_all_years.json\" if (self.processed_data_dir / \"indicators_all_years.json\").exists() else None
        }
        
        # Save updated index
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(data_index, f, ensure_ascii=False, indent=2)
        
        logger.info(f\"Updated comprehensive data index: {index_path}\")

    def copy_data_to_github_pages_location(self):
        """Copy data to locations accessible by GitHub Pages"""
        logger.info(\"Copying data to GitHub Pages accessible locations...\")
        
        # Create data directory in public folder for GitHub Pages
        github_pages_data_dir = self.base_dir / \"public\" / \"data\"
        github_pages_data_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy processed data files to GitHub Pages location
        for file_name in self.data_files_to_deploy:
            src_path = self.processed_data_dir / file_name
            if src_path.exists():
                dest_path = github_pages_data_dir / file_name
                shutil.copy2(src_path, dest_path)
                logger.info(f\"Copied {src_path.name} to GitHub Pages location: {dest_path}\")
        
        # Also copy to frontend public data location
        frontend_public_data_dir = self.base_dir / \"frontend\" / \"public\" / \"data\"
        frontend_public_data_dir.mkdir(parents=True, exist_ok=True)
        
        for file_name in self.data_files_to_deploy:
            src_path = self.processed_data_dir / file_name
            if src_path.exists():
                dest_path = frontend_public_data_dir / file_name
                shutil.copy2(src_path, dest_path)
                logger.info(f\"Copied {src_path.name} to frontend public location: {dest_path}\")

    def update_github_pages_config(self):
        """Update GitHub Pages configuration to ensure data paths are correct"""
        logger.info(\"Updating GitHub Pages configuration...\")
        
        # Update the existing year-specific JSON files with references to new data
        for year in range(2022, 2026):  # 2022 to 2025
            year_data_path = self.frontend_data_dir / f\"data_index_{year}.json\"
            if year_data_path.exists():
                with open(year_data_path, 'r', encoding='utf-8') as f:
                    year_data = json.load(f)
                
                # Add references to the processed data
                if 'data_sources' not in year_data:
                    year_data['data_sources'] = {}
                
                year_data['data_sources']['processed_data'] = {
                    \"budget_execution_csv\": \"/data/budget_execution_all_years.csv\",
                    \"indicators_csv\": \"/data/indicators_all_years.csv\",
                    \"budget_execution_json\": \"/data/budget_execution_all_years.json\",
                    \"indicators_json\": \"/data/indicators_all_years.json\"
                }
                
                # Save updated year data
                with open(year_data_path, 'w', encoding='utf-8') as f:
                    json.dump(year_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f\"Updated year {year} data index with processed data references\")

    def deploy_for_github_pages(self):
        """Deploy data for GitHub Pages"""
        logger.info(\"Deploying data for GitHub Pages...\")
        
        # Prepare data for frontend
        self.prepare_data_for_frontend()
        
        # Update the comprehensive data index
        self.update_frontend_data_index()
        
        # Copy data to GitHub Pages accessible locations
        self.copy_data_to_github_pages_location()
        
        # Update GitHub Pages configuration
        self.update_github_pages_config()
        
        logger.info(\"GitHub Pages data deployment completed successfully\")

def main():
    """Main function to run the data deployment process"""
    deployer = DataDeployment()
    deployer.deploy_for_github_pages()
    
    print(f\"\\nGitHub Pages data deployment completed!\")
    print(f\"Data copied to GitHub Pages locations:\")
    print(f\"  - {deployer.base_dir}/public/data/\")
    print(f\"  - {deployer.base_dir}/frontend/public/data/\")
    print(f\"Updated data index files in {deployer.frontend_data_dir}/\")

if __name__ == \"__main__\":
    main()