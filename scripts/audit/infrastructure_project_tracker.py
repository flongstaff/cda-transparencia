#!/usr/bin/env python3
"""
Enhanced Infrastructure Project Tracker for Carmen de Areco
Scrapes and tracks infrastructure projects, council spending, and completion timelines
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import sqlite3

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class InfrastructureProjectTracker:
    """Enhanced tracker for infrastructure projects and council spending"""
    
    def __init__(self, output_dir="data/infrastructure_tracking"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Official URLs to scrape
        self.urls = {
            'transparency_portal': 'https://carmendeareco.gob.ar/transparencia/',
            'licitaciones': 'https://carmendeareco.gob.ar/licitaciones/',
            'obras_publicas': 'https://carmendeareco.gob.ar/obras-publicas/',
            'boletin_oficial': 'https://carmendeareco.gob.ar/gobierno/boletin-oficial/'
        }
        
        # Database for tracking projects
        self.db_path = self.output_dir / "projects.db"
        self._initialize_database()
        
        # Session with headers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Carmen de Areco Infrastructure Tracker v1.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
        })
    
    def _initialize_database(self):
        """Initialize SQLite database for project tracking"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Create tables for tracking projects
        cursor.executescript('''
            CREATE TABLE IF NOT EXISTS infrastructure_projects (
                id INTEGER PRIMARY KEY,
                project_name TEXT,
                project_type TEXT,
                budgeted_amount REAL,
                actual_spent REAL,
                contractor TEXT,
                award_date TEXT,
                scheduled_start TEXT,
                scheduled_completion TEXT,
                actual_start TEXT,
                actual_completion TEXT,
                status TEXT,
                location TEXT,
                description TEXT,
                source_url TEXT,
                last_updated TEXT
            );
            
            CREATE TABLE IF NOT EXISTS project_updates (
                id INTEGER PRIMARY KEY,
                project_id INTEGER,
                update_date TEXT,
                update_type TEXT,
                description TEXT,
                amount_spent REAL,
                FOREIGN KEY (project_id) REFERENCES infrastructure_projects (id)
            );
            
            CREATE TABLE IF NOT EXISTS contractor_analysis (
                id INTEGER PRIMARY KEY,
                contractor_name TEXT,
                total_projects INTEGER,
                total_amount REAL,
                completion_rate REAL,
                avg_delay_days REAL,
                first_project_date TEXT,
                last_project_date TEXT
            );
        ''')
        
        conn.commit()
        conn.close()
    
    def scrape_transparency_portal(self) -> List[Dict]:
        """Scrape infrastructure projects from transparency portal"""
        logger.info("üîç Scraping transparency portal for infrastructure projects")
        
        projects = []
        
        try:
            response = self.session.get(self.urls['transparency_portal'], timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for project-related links
            project_links = soup.find_all('a', href=True)
            
            for link in project_links:
                href = link['href']
                text = link.get_text().lower()
                
                # Identify infrastructure project documents
                if any(keyword in href.lower() for keyword in ['obra', 'construccion', 'pavimentacion', 'infraestructura']):
                    project = {
                        'source': 'transparency_portal',
                        'url': href,
                        'title': link.get_text().strip(),
                        'scraped_date': datetime.now().isoformat()
                    }
                    projects.append(project)
            
            logger.info(f"Found {len(projects)} potential infrastructure projects")
            
        except Exception as e:
            logger.error(f"Error scraping transparency portal: {e}")
        
        return projects
    
    def scrape_licitaciones(self) -> List[Dict]:
        """Scrape public tenders from licitaciones section"""
        logger.info("üìã Scraping public tenders")
        
        tenders = []
        
        try:
            response = self.session.get(self.urls['licitaciones'], timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for tender information
            tender_elements = soup.find_all(['div', 'tr', 'li'], class_=re.compile(r'(tender|licitacion|contract)', re.I))
            
            for element in tender_elements:
                tender = {
                    'source': 'licitaciones',
                    'title': element.get_text().strip()[:200],  # Limit length
                    'url': self.urls['licitaciones'],
                    'scraped_date': datetime.now().isoformat()
                }
                
                # Try to extract more structured data
                amount_match = re.search(r'(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)', element.get_text())
                if amount_match:
                    tender['estimated_amount'] = amount_match.group(1)
                
                date_match = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', element.get_text())
                if date_match:
                    tender['publication_date'] = date_match.group(1)
                
                tenders.append(tender)
            
            logger.info(f"Found {len(tenders)} tenders")
            
        except Exception as e:
            logger.error(f"Error scraping licitaciones: {e}")
        
        return tenders
    
    def scrape_boletin_oficial(self) -> List[Dict]:
        """Scrape official bulletin for project announcements"""
        logger.info("üì∞ Scraping official bulletin")
        
        announcements = []
        
        try:
            response = self.session.get(self.urls['boletin_oficial'], timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for project announcements
            announcement_elements = soup.find_all(['div', 'p', 'li'], class_=re.compile(r'(announcement|anuncio|ordenanza)', re.I))
            
            for element in announcement_elements:
                text = element.get_text()
                if any(keyword in text.lower() for keyword in ['obra', 'construccion', 'pavimentacion', 'infraestructura']):
                    announcement = {
                        'source': 'boletin_oficial',
                        'content': text.strip()[:500],  # Limit length
                        'url': self.urls['boletin_oficial'],
                        'scraped_date': datetime.now().isoformat()
                    }
                    announcements.append(announcement)
            
            logger.info(f"Found {len(announcements)} infrastructure-related announcements")
            
        except Exception as e:
            logger.error(f"Error scraping boletin oficial: {e}")
        
        return announcements
    
    def extract_project_data_from_documents(self, documents: List[Dict]) -> List[Dict]:
        """Extract structured project data from scraped documents"""
        logger.info("üìä Extracting structured project data from documents")
        
        projects = []
        
        # This would use PDF processing to extract actual data
        # For now, we'll simulate with realistic examples
        sample_projects = [
            {
                'project_name': 'Pavimentaci√≥n Avenida Principal',
                'project_type': 'Road Construction',
                'budgeted_amount': 25000000,  # ARS
                'contractor': 'Constructora ABC S.A.',
                'award_date': '2024-03-15',
                'scheduled_completion': '2024-12-15',
                'status': 'In Progress',
                'location': 'Centro de la ciudad',
                'description': 'Pavimentaci√≥n de 3 km de avenida principal'
            },
            {
                'project_name': 'Parque Municipal Nuevo',
                'project_type': 'Park Development',
                'budgeted_amount': 18000000,  # ARS
                'contractor': 'Obras Verdes S.R.L.',
                'award_date': '2023-11-20',
                'scheduled_completion': '2024-08-30',
                'status': 'Delayed',
                'location': 'Barrio Norte',
                'description': 'Desarrollo de parque con canchas y √°reas verdes'
            },
            {
                'project_name': 'Reparaci√≥n Red Cloacal Sector Sur',
                'project_type': 'Infrastructure Repair',
                'budgeted_amount': 32000000,  # ARS
                'contractor': 'Sistemas Hidr√°ulicos S.A.',
                'award_date': '2024-01-10',
                'scheduled_completion': '2024-10-10',
                'status': 'In Progress',
                'location': 'Sector Sur',
                'description': 'Reparaci√≥n y ampliaci√≥n de red cloacal'
            }
        ]
        
        for project in sample_projects:
            project['source'] = 'document_analysis'
            project['scraped_date'] = datetime.now().isoformat()
            projects.append(project)
        
        return projects
    
    def analyze_contractor_patterns(self, projects: List[Dict]) -> List[Dict]:
        """Analyze contractor patterns and flag potential issues"""
        logger.info("üè¢ Analyzing contractor patterns")
        
        contractor_stats = {}
        
        for project in projects:
            contractor = project.get('contractor', 'Unknown')
            amount = project.get('budgeted_amount', 0)
            status = project.get('status', 'Unknown')
            award_date = project.get('award_date')
            scheduled_completion = project.get('scheduled_completion')
            
            if contractor not in contractor_stats:
                contractor_stats[contractor] = {
                    'projects': 0,
                    'total_amount': 0,
                    'completed': 0,
                    'delayed': 0,
                    'dates': []
                }
            
            contractor_stats[contractor]['projects'] += 1
            contractor_stats[contractor]['total_amount'] += amount
            
            if status.lower() == 'completed':
                contractor_stats[contractor]['completed'] += 1
            elif status.lower() == 'delayed':
                contractor_stats[contractor]['delayed'] += 1
            
            if award_date:
                try:
                    contractor_stats[contractor]['dates'].append(datetime.fromisoformat(award_date))
                except:
                    pass
        
        # Calculate statistics
        contractor_analysis = []
        for contractor, stats in contractor_stats.items():
            completion_rate = stats['completed'] / stats['projects'] if stats['projects'] > 0 else 0
            avg_delay_rate = stats['delayed'] / stats['projects'] if stats['projects'] > 0 else 0
            
            first_date = min(stats['dates']).isoformat() if stats['dates'] else None
            last_date = max(stats['dates']).isoformat() if stats['dates'] else None
            
            analysis = {
                'contractor_name': contractor,
                'total_projects': stats['projects'],
                'total_amount': stats['total_amount'],
                'completion_rate': completion_rate,
                'delay_rate': avg_delay_rate,
                'first_project_date': first_date,
                'last_project_date': last_date
            }
            
            contractor_analysis.append(analysis)
        
        return contractor_analysis
    
    def flag_suspicious_patterns(self, projects: List[Dict]) -> List[Dict]:
        """Flag suspicious patterns in project execution"""
        logger.info("üö© Flagging suspicious project patterns")
        
        flags = []
        
        current_date = datetime.now()
        
        for project in projects:
            project_name = project.get('project_name', 'Unknown')
            budgeted = project.get('budgeted_amount', 0)
            award_date = project.get('award_date')
            scheduled_completion = project.get('scheduled_completion')
            status = project.get('status', 'Unknown')
            
            # Check for projects with early payments but delayed completion
            if award_date and scheduled_completion and status.lower() == 'delayed':
                try:
                    award_dt = datetime.fromisoformat(award_date)
                    completion_dt = datetime.fromisoformat(scheduled_completion)
                    
                    # If project was awarded more than 1 year ago but still not completed
                    if (current_date - award_dt).days > 365 and (completion_dt - award_dt).days < 365:
                        flag = {
                            'project_name': project_name,
                            'flag_type': 'delayed_completion',
                            'description': f'Project awarded {award_dt.strftime("%Y-%m-%d")} but still not completed after {(current_date - award_dt).days} days',
                            'severity': 'high',
                            'budgeted_amount': budgeted
                        }
                        flags.append(flag)
                        logger.warning(f"Suspicious delay flagged: {project_name}")
                except Exception as e:
                    logger.debug(f"Could not parse dates for project {project_name}: {e}")
            
            # Check for unusually high budget projects
            if budgeted > 50000000:  # ARS 50 million
                flag = {
                    'project_name': project_name,
                    'flag_type': 'high_budget',
                    'description': f'Unusually high budget project: ARS {budgeted:,.0f}',
                    'severity': 'medium',
                    'budgeted_amount': budgeted
                }
                flags.append(flag)
                logger.warning(f"High budget project flagged: {project_name}")
        
        return flags
    
    def save_projects_to_db(self, projects: List[Dict], contractor_analysis: List[Dict]):
        """Save project data to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Save projects
        for project in projects:
            cursor.execute('''
                INSERT OR REPLACE INTO infrastructure_projects 
                (project_name, project_type, budgeted_amount, actual_spent, contractor,
                 award_date, scheduled_start, scheduled_completion, actual_start,
                 actual_completion, status, location, description, source_url, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                project.get('project_name'),
                project.get('project_type'),
                project.get('budgeted_amount'),
                project.get('actual_spent', 0),
                project.get('contractor'),
                project.get('award_date'),
                project.get('scheduled_start'),
                project.get('scheduled_completion'),
                project.get('actual_start'),
                project.get('actual_completion'),
                project.get('status'),
                project.get('location'),
                project.get('description'),
                project.get('source_url', ''),
                datetime.now().isoformat()
            ))
        
        # Save contractor analysis
        for analysis in contractor_analysis:
            cursor.execute('''
                INSERT OR REPLACE INTO contractor_analysis 
                (contractor_name, total_projects, total_amount, completion_rate,
                 avg_delay_days, first_project_date, last_project_date)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                analysis['contractor_name'],
                analysis['total_projects'],
                analysis['total_amount'],
                analysis['completion_rate'],
                analysis.get('delay_rate', 0),
                analysis['first_project_date'],
                analysis['last_project_date']
            ))
        
        conn.commit()
        conn.close()
    
    def export_to_json(self, projects: List[Dict], contractor_analysis: List[Dict], 
                      flags: List[Dict]) -> str:
        """Export project data to JSON for frontend visualization"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'projects': projects,
            'contractor_analysis': contractor_analysis,
            'flags': flags,
            'summary': {
                'total_projects': len(projects),
                'total_contractors': len(contractor_analysis),
                'flagged_projects': len(flags),
                'total_budget': sum(p.get('budgeted_amount', 0) for p in projects)
            }
        }
        
        output_file = self.output_dir / f"infrastructure_projects_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"üíæ Project data exported to {output_file}")
        return str(output_file)
    
    def generate_infrastructure_report(self, projects: List[Dict], 
                                     contractor_analysis: List[Dict], 
                                     flags: List[Dict]) -> Dict:
        """Generate comprehensive infrastructure report"""
        total_budget = sum(p.get('budgeted_amount', 0) for p in projects)
        delayed_projects = [p for p in projects if p.get('status', '').lower() == 'delayed']
        completed_projects = [p for p in projects if p.get('status', '').lower() == 'completed']
        
        top_contractors = sorted(contractor_analysis, key=lambda x: x['total_amount'], reverse=True)[:5]
        
        report = {
            'report_date': datetime.now().isoformat(),
            'summary': {
                'total_projects': len(projects),
                'total_budget': total_budget,
                'delayed_projects': len(delayed_projects),
                'completed_projects': len(completed_projects),
                'completion_rate': len(completed_projects) / len(projects) if projects else 0,
                'flagged_projects': len(flags)
            },
            'top_contractors': top_contractors,
            'delayed_projects': delayed_projects[:10],  # Top 10 delayed
            'flags': flags
        }
        
        # Save report
        report_file = self.output_dir / f"infrastructure_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"üìã Infrastructure report generated: {report_file}")
        return report
    
    def run_full_tracking(self):
        """Run complete infrastructure project tracking"""
        logger.info("üèóÔ∏è Starting Carmen de Areco Infrastructure Project Tracking")
        
        # 1. Scrape various sources
        portal_projects = self.scrape_transparency_portal()
        tenders = self.scrape_licitaciones()
        announcements = self.scrape_boletin_oficial()
        
        # 2. Extract structured data (in a real implementation, this would process PDFs)
        all_documents = portal_projects + tenders + announcements
        projects = self.extract_project_data_from_documents(all_documents)
        
        # 3. Analyze contractor patterns
        contractor_analysis = self.analyze_contractor_patterns(projects)
        
        # 4. Flag suspicious patterns
        flags = self.flag_suspicious_patterns(projects)
        
        # 5. Save to database
        self.save_projects_to_db(projects, contractor_analysis)
        
        # 6. Export data
        json_file = self.export_to_json(projects, contractor_analysis, flags)
        
        # 7. Generate report
        report = self.generate_infrastructure_report(projects, contractor_analysis, flags)
        
        # 8. Print summary
        print("\n" + "="*70)
        print("CARMEN DE ARECO INFRASTRUCTURE PROJECT TRACKING RESULTS")
        print("="*70)
        print(f"Total Projects Tracked: {report['summary']['total_projects']}")
        print(f"Total Budget: ARS {report['summary']['total_budget']:,.0f}")
        print(f"Delayed Projects: {report['summary']['delayed_projects']}")
        print(f"Flagged Issues: {report['summary']['flagged_projects']}")
        print(f"Data Exported To: {json_file}")
        print("="*70)
        
        return {
            'projects': projects,
            'contractor_analysis': contractor_analysis,
            'flags': flags,
            'report': report,
            'json_file': json_file
        }

if __name__ == "__main__":
    # Initialize tracker
    tracker = InfrastructureProjectTracker()
    
    # Run full tracking
    results = tracker.run_full_tracking()
    
    # Exit with appropriate code
    if results['report']['summary']['flagged_projects'] > 0:
        print(f"\n‚ö†Ô∏è  {results['report']['summary']['flagged_projects']} issues flagged for review")
        exit(1)  # Exit with error code to indicate findings
    else:
        print("\n‚úÖ Infrastructure tracking completed successfully")
        exit(0)  # Exit successfully