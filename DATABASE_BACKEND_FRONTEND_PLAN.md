# Database, Backend, and Frontend Integration Plan

## ğŸ¯ Project Overview

The Carmen de Areco Transparency Portal aims to provide citizens with easy access to government financial data, organized by year with reliable data sources and automated collection.

## ğŸ—ï¸ Current Architecture

### Frontend (React + TypeScript + Vite)
- Located in `/frontend/`
- Year-based data switching implemented
- Uses mock/generated data for demonstration
- Data services for year management and validation

### Backend (Node.js + Express + PostgreSQL)
- Located in `/backend/`
- API endpoints defined for all data types
- Database schema created but not populated
- Models and controllers implemented

### Data Organization
- Local data stored in `/data/source_materials/`
- Organized by year (2018-2025) and category
- 700+ PDF documents available

## ğŸ—ƒï¸ Database Backend Plan

### 1. PostgreSQL Database Setup

#### Connection Configuration
- Create `.env` file in `/backend/`:
```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=transparency_portal
DB_USER=postgres
DB_PASSWORD=your_password
PORT=3000
NODE_ENV=development
```

#### Database Initialization
1. Create database:
```sql
CREATE DATABASE transparency_portal;
```

2. Run initialization script:
```bash
cd backend
psql -U postgres -d transparency_portal -f init.sql
```

### 2. Data Population Strategy

#### Phase 1: Import Existing Local Data
- Parse PDF documents in `/data/source_materials/`
- Extract structured data using document analysis
- Populate database tables with extracted information

#### Phase 2: Automated Data Collection
- Implement web crawlers for official site and Web Archive
- Schedule regular data synchronization
- Validate and cross-reference collected data

#### Phase 3: Live Data Integration
- Connect frontend to backend API
- Implement real-time data fetching
- Add data validation and caching

## ğŸ”Œ Backend API Implementation

### Current Status
- âœ… API routes defined for all data types
- âœ… Controllers and models created
- âŒ Database not populated with real data
- âŒ API endpoints not tested with real data

### Next Steps
1. Populate database with sample data
2. Test all API endpoints
3. Implement data validation in controllers
4. Add error handling and logging
5. Implement authentication (if needed)

### Example API Usage
```bash
# Get all property declarations
curl http://localhost:3000/api/declarations

# Get declarations for a specific year
curl http://localhost:3000/api/declarations/year/2024

# Get declarations for a specific official
curl http://localhost:3000/api/declarations/official/"John Doe"
```

## ğŸ–¥ï¸ Frontend Integration

### Current Status
- âœ… Year-based data switching implemented
- âœ… Mock data services working
- âŒ Backend API integration not implemented
- âŒ Real data not connected

### Integration Plan

#### 1. Update Data Services
Modify `/frontend/src/services/DataService.ts` to fetch from backend API:

```typescript
// Before: Using mock data
const budgetData2024 = await import('../data/budget-data');

// After: Using API calls
const response = await fetch('/api/reports/year/2024');
const budgetData2024 = await response.json();
```

#### 2. Environment Configuration
Update `/frontend/.env`:
```env
VITE_API_URL=http://localhost:3000/api
```

#### 3. API Service Implementation
Create `/frontend/src/services/ApiService.ts`:
```typescript
const API_BASE_URL = import.meta.env.VITE_API_URL || '/api';

export const apiService = {
  async getDeclarations(year?: string) {
    const url = year 
      ? `${API_BASE_URL}/declarations/year/${year}`
      : `${API_BASE_URL}/declarations`;
    const response = await fetch(url);
    return response.json();
  },

  async getSalaries(year?: string) {
    const url = year 
      ? `${API_BASE_URL}/salaries/year/${year}`
      : `${API_BASE_URL}/salaries`;
    const response = await fetch(url);
    return response.json();
  },

  // Similar methods for other data types...
};
```

## ğŸ“ Data Organization Strategy

### 1. Year-Based Directory Structure
```
/data/source_materials/
â”œâ”€â”€ 2022/
â”‚   â”œâ”€â”€ budget/
â”‚   â”œâ”€â”€ tenders/
â”‚   â”œâ”€â”€ declarations/
â”‚   â””â”€â”€ reports/
â”œâ”€â”€ 2023/
â”œâ”€â”€ 2024/
â”œâ”€â”€ 2025/
â””â”€â”€ cross_year/
    â”œâ”€â”€ officials/
    â”œâ”€â”€ contracts/
    â””â”€â”€ assets/
```

### 2. Document Categorization
- Budget Documents: `presupuesto`, `ejecucion`, `balance`
- Contracts: `licitacion`, `contrato`, `adjudicacion`
- Declarations: `declaracion`, `patrimonio`, `ddjj`
- Reports: `informe`, `auditoria`, `memoria`
- Resolutions: `resolucion`, `decreto`, `ordenanza`

### 3. Metadata Management
Each document should have metadata:
- Year
- Category
- Title
- Date
- Source URL
- File path
- Hash for integrity verification

## ğŸ•·ï¸ Automated Data Collection

### 1. Official Site Crawler
Script: `/frontend/scripts/data-collector.js`
- Downloads documents from `carmendeareco.gob.ar/transparencia/`
- Categorizes and organizes by year/type
- Updates local database

### 2. Web Archive Spider
Script: `/frontend/scripts/web-spider.js`
- Crawls `archive.org` snapshots
- Retrieves historical versions
- Cross-references with current data

### 3. Data Synchronization
Script: `/frontend/scripts/data-sync.js`
- Runs daily synchronization
- Compares document hashes
- Reports discrepancies
- Updates database with new/changed data

## ğŸš€ Implementation Roadmap

### Phase 1: Database and Backend (Week 1)
1. Set up PostgreSQL database
2. Run initialization script
3. Populate with sample data
4. Test all API endpoints
5. Implement basic data validation

### Phase 2: Frontend Integration (Week 2)
1. Update data services to use API
2. Implement year-based data fetching
3. Add loading states and error handling
4. Test all pages with real data
5. Implement data validation in frontend

### Phase 3: Data Organization (Week 3)
1. Organize local documents by year/category
2. Create document metadata system
3. Implement document categorization
4. Set up automated collection scripts
5. Test data synchronization

### Phase 4: Advanced Features (Week 4)
1. Implement cross-source validation
2. Add data integrity monitoring
3. Create backup and restore system
4. Implement advanced search
5. Add data export capabilities

## ğŸ› ï¸ Technical Requirements

### Backend
- Node.js v16+
- PostgreSQL v12+
- Express.js
- Sequelize ORM

### Frontend
- Node.js v16+
- npm v8+
- Modern browser support

### Data Processing
- PDF parsing libraries
- Web crawling tools
- Data validation utilities

## ğŸ“Š Data Flow Architecture

```mermaid
graph TD
    A[Official Site] --> B[Data Collector]
    C[Web Archive] --> D[Web Spider]
    E[Local Documents] --> F[Document Processor]
    
    B --> G[Database]
    D --> G
    F --> G
    
    G --> H[Backend API]
    H --> I[Frontend Services]
    I --> J[Year-Based Components]
    
    J --> K[UI Pages]
```

## ğŸ” Validation and Monitoring

### Data Validation
- Cross-source verification
- Temporal consistency checks
- Completeness scoring
- Hash-based integrity verification

### Monitoring
- Data source health checks
- Sync status monitoring
- Error detection and alerting
- Performance metrics

## ğŸ¯ Success Criteria

1. âœ… Database populated with real municipal data
2. âœ… Backend API serving real data
3. âœ… Frontend fetching data from backend
4. âœ… Year switching works with real data
5. âœ… Data validation and cross-referencing implemented
6. âœ… Automated data collection working
7. âœ… Document preview system functional

## ğŸš¨ Risks and Mitigations

### Risk: Official site changes structure
**Mitigation:** Implement flexible parsing with Web Archive fallback

### Risk: Data inconsistency between sources
**Mitigation:** Cross-reference and flag discrepancies

### Risk: Large document processing overhead
**Mitigation:** Implement caching and lazy loading

### Risk: Database performance issues
**Mitigation:** Add indexing and query optimization

## ğŸ“… Timeline

| Week | Focus Area | Deliverables |
|------|------------|--------------|
| 1 | Database & Backend | Working API with sample data |
| 2 | Frontend Integration | UI connected to real backend |
| 3 | Data Organization | Organized documents and metadata |
| 4 | Advanced Features | Validation, monitoring, backups |

## ğŸ’¡ Key Benefits

- âœ… Reliable data from multiple sources
- âœ… Year-based data switching with real data
- âœ… Automated data collection and synchronization
- âœ… Data integrity verification
- âœ… Document preview with contextual data
- âœ… Scalable architecture for future expansion